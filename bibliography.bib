@misc{goodfellow14,
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	archiveprefix = {arXiv},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	comment = {published = 2014-06-10T18:58:17Z, updated = 2014-06-10T18:58:17Z},
	eprint = {1406.2661v1},
	localfile = {bibliography_resources/goodfellow - Generative Adversarial Networks.pdf},
	month = jun,
	primaryclass = {stat.ML},
	title = {{Generative Adversarial Networks}},
	url = {http://arxiv.org/abs/1406.2661v1; http://arxiv.org/pdf/1406.2661v1},
	x-fetchedfrom = {arXiv.org},
	year = {2014}
}

@inproceedings{drover18,
	abstract = {3D pose estimation from a single image is a challenging task in computer vision. We present a weakly supervised approach to estimate 3D pose points, given only 2D pose landmarks. Our method does not require correspondences between 2D and 3D points to build explicit 3D priors. We utilize an adversarial framework to impose a prior on the 3D structure, learned solely from their random 2D projections. Given a set of 2D pose landmarks, the generator network hypothesizes their depths to obtain a 3D skeleton. We propose a novel Random Projection layer, which randomly projects the generated 3D skeleton and sends the resulting 2D pose to the discriminator. The discriminator improves by discriminating between the generated poses and pose samples from a real distribution of 2D poses. Training does not require correspondence between the 2D inputs to either the generator or the discriminator. We apply our approach to the task of 3D human pose estimation. Results on Human3.6M dataset demonstrates that our approach outperforms many previous supervised and weakly supervised approaches.},
	author = {Drover, Dylan and {M. V}, Rohith and Chen, Ching-Hang and Agrawal, Amit and Tyagi, Ambrish and Huynh, Cong Phuoc},
	booktitle = {{Computer Vision -- ECCV 2018 Workshops}},
	isbn = {978-3-030-11018-5},
	pages = {78--94},
	publisher = {Springer International Publishing},
	title = {{Can 3D Pose Be Learned from 2D Projections Alone?}},
	year = {2019}
}

@article{ionescu14,
	author = {Ionescu, Catalin and Papava, Dragos and Olaru, Vlad and Sminchisescu, Cristian},
	doi = {10.1109/tpami.2013.248},
	issn = {2160-9292},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	localfile = {bibliography_resources/ionescu - Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments.pdf},
	month = {Jul},
	number = {7},
	pages = {1325--1339},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	title = {{Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments}},
	url = {http://dx.doi.org/10.1109/TPAMI.2013.248},
	volume = {36},
	x-fetchedfrom = {DOI},
	year = {2014}
}

@misc{sun17,
	abstract = {  Regression based methods are not performing as well as detection based
methods for human pose estimation. A central problem is that the structural
information in the pose is not well exploited in the previous regression
methods. In this work, we propose a structure-aware regression approach. It
adopts a reparameterized pose representation using bones instead of joints. It
exploits the joint connection structure to define a compositional loss function
that encodes the long range interactions in the pose. It is simple, effective,
and general for both 2D and 3D pose estimation in a unified setting.
Comprehensive evaluation validates the effectiveness of our approach. It
significantly advances the state-of-the-art on Human3.6M and is competitive
with state-of-the-art results on MPII.
},
	archiveprefix = {arXiv},
	author = {Sun, Xiao and Shang, Jiaxiang and Liang, Shuang and Wei, Yichen},
	comment = {published = 2017-04-01T11:59:41Z, updated = 2017-08-02T03:31:39Z, Accepted by International Conference on Computer Vision (ICCV) 2017},
	eprint = {1704.00159v3},
	localfile = {bibliography_resources/sun - Compositional Human Pose Regression.pdf},
	month = aug,
	primaryclass = {cs.CV},
	title = {{Compositional Human Pose Regression}},
	url = {http://arxiv.org/abs/1704.00159v3; http://arxiv.org/pdf/1704.00159v3},
	x-fetchedfrom = {arXiv.org},
	year = {2017}
}

@misc{martinez17,
	abstract = {  Following the success of deep convolutional networks, state-of-the-art
methods for 3d human pose estimation have focused on deep end-to-end systems
that predict 3d joint locations given raw image pixels. Despite their excellent
performance, it is often not easy to understand whether their remaining error
stems from a limited 2d pose (visual) understanding, or from a failure to map
2d poses into 3-dimensional positions. With the goal of understanding these
sources of error, we set out to build a system that given 2d joint locations
predicts 3d positions. Much to our surprise, we have found that, with current
technology, "lifting" ground truth 2d joint locations to 3d space is a task
that can be solved with a remarkably low error rate: a relatively simple deep
feed-forward network outperforms the best reported result by about 30\% on
Human3.6M, the largest publicly available 3d pose estimation benchmark.
Furthermore, training our system on the output of an off-the-shelf
state-of-the-art 2d detector (\ie, using images as input) yields state of the
art results -- this includes an array of systems that have been trained
end-to-end specifically for this task. Our results indicate that a large
portion of the error of modern deep 3d pose estimation systems stems from their
visual analysis, and suggests directions to further advance the state of the
art in 3d human pose estimation.
},
	archiveprefix = {arXiv},
	author = {Martinez, Julieta and Hossain, Rayat and Romero, Javier and Little, James J.},
	comment = {published = 2017-05-08T21:48:37Z, updated = 2017-08-04T18:36:24Z, Accepted to ICCV 17},
	eprint = {1705.03098v2},
	localfile = {./bibliography_resources/martinez - a simple baseline for 3d hume pose estimation.pdf},
	month = aug,
	primaryclass = {cs.CV},
	title = {{A simple yet effective baseline for 3d human pose estimation}},
	url = {http://arxiv.org/abs/1705.03098v2; http://arxiv.org/pdf/1705.03098v2},
	x-fetchedfrom = {arXiv.org},
	year = {2017}
}

@misc{zhou16,
	abstract = {  This paper addresses the challenge of 3D full-body human pose estimation from
a monocular image sequence. Here, two cases are considered: (i) the image
locations of the human joints are provided and (ii) the image locations of
joints are unknown. In the former case, a novel approach is introduced that
integrates a sparsity-driven 3D geometric prior and temporal smoothness. In the
latter case, the former case is extended by treating the image locations of the
joints as latent variables. A deep fully convolutional network is trained to
predict the uncertainty maps of the 2D joint locations. The 3D pose estimates
are realized via an Expectation-Maximization algorithm over the entire
sequence, where it is shown that the 2D joint location uncertainties can be
conveniently marginalized out during inference. Empirical evaluation on the
Human3.6M dataset shows that the proposed approaches achieve greater 3D pose
estimation accuracy over state-of-the-art baselines. Further, the proposed
approach outperforms a publicly available 2D pose estimation baseline on the
challenging PennAction dataset.
},
	archiveprefix = {arXiv},
	author = {Zhou, Xiaowei and Zhu, Menglong and Leonardos, Spyridon and Derpanis, Kosta and Daniilidis, Kostas},
	comment = {published = 2015-11-30T19:41:06Z, updated = 2016-04-28T14:53:43Z, Published in CVPR2016},
	eprint = {1511.09439v2},
	localfile = {bibliography_resources/zhou - Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video.pdf},
	month = apr,
	primaryclass = {cs.CV},
	title = {{Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video}},
	url = {http://arxiv.org/abs/1511.09439v2; http://arxiv.org/pdf/1511.09439v2},
	x-fetchedfrom = {arXiv.org},
	year = {2016}
}

@misc{tekin17,
	abstract = {  Most recent approaches to monocular 3D human pose estimation rely on Deep
Learning. They typically involve regressing from an image to either 3D joint
coordinates directly or 2D joint locations from which 3D coordinates are
inferred. Both approaches have their strengths and weaknesses and we therefore
propose a novel architecture designed to deliver the best of both worlds by
performing both simultaneously and fusing the information along the way. At the
heart of our framework is a trainable fusion scheme that learns how to fuse the
information optimally instead of being hand-designed. This yields significant
improvements upon the state-of-the-art on standard 3D human pose estimation
benchmarks.
},
	archiveprefix = {arXiv},
	author = {Tekin, Bugra and M{\'a}rquez-Neila, Pablo and Salzmann, Mathieu and Fua, Pascal},
	comment = {published = 2016-11-17T14:40:53Z, updated = 2017-04-10T10:49:00Z},
	eprint = {1611.05708v3},
	localfile = {bibliography_resources/tekin - learning to fuse 2d and 3d image cues.pdf},
	month = apr,
	primaryclass = {cs.CV},
	title = {{Learning to Fuse 2D and 3D Image Cues for Monocular Body Pose Estimation}},
	url = {http://arxiv.org/abs/1611.05708v3; http://arxiv.org/pdf/1611.05708v3},
	x-fetchedfrom = {arXiv.org},
	year = {2017}
}

@misc{ioffe15,
	abstract = {  Training Deep Neural Networks is complicated by the fact that the
distribution of each layer's inputs changes during training, as the parameters
of the previous layers change. This slows down the training by requiring lower
learning rates and careful parameter initialization, and makes it notoriously
hard to train models with saturating nonlinearities. We refer to this
phenomenon as internal covariate shift, and address the problem by normalizing
layer inputs. Our method draws its strength from making normalization a part of
the model architecture and performing the normalization for each training
mini-batch. Batch Normalization allows us to use much higher learning rates and
be less careful about initialization. It also acts as a regularizer, in some
cases eliminating the need for Dropout. Applied to a state-of-the-art image
classification model, Batch Normalization achieves the same accuracy with 14
times fewer training steps, and beats the original model by a significant
margin. Using an ensemble of batch-normalized networks, we improve upon the
best published result on ImageNet classification: reaching 4.9\% top-5
validation error (and 4.8\% test error), exceeding the accuracy of human raters.
},
	archiveprefix = {arXiv},
	author = {Ioffe, Sergey and Szegedy, Christian},
	comment = {published = 2015-02-11T01:44:18Z, updated = 2015-03-02T20:44:12Z},
	eprint = {1502.03167v3},
	localfile = {bibliography_resources/ioffe - Batch Normalization.pdf},
	month = mar,
	primaryclass = {cs.LG},
	title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
	url = {http://arxiv.org/abs/1502.03167v3; http://arxiv.org/pdf/1502.03167v3},
	x-fetchedfrom = {arXiv.org},
	year = {2015}
}

@misc{yasin16,
	abstract = {  One major challenge for 3D pose estimation from a single RGB image is the
acquisition of sufficient training data. In particular, collecting large
amounts of training data that contain unconstrained images and are annotated
with accurate 3D poses is infeasible. We therefore propose to use two
independent training sources. The first source consists of images with
annotated 2D poses and the second source consists of accurate 3D motion capture
data. To integrate both sources, we propose a dual-source approach that
combines 2D pose estimation with efficient and robust 3D pose retrieval. In our
experiments, we show that our approach achieves state-of-the-art results and is
even competitive when the skeleton structure of the two sources differ
substantially.
},
	archiveprefix = {arXiv},
	author = {Yasin, Hashim and Iqbal, Umar and Kr{\"u}ger, Bj{\"o}rn and Weber, Andreas and Gall, Juergen},
	comment = {published = 2015-09-22T18:38:16Z, updated = 2016-03-27T11:43:06Z, Accepted to CVPR 2016. The source code and models are publicly available. Title changed from the previous version},
	eprint = {1509.06720v2},
	localfile = {bibliography_resources/yasin - a dual-source approach for 3d pose estimation.pdf},
	month = mar,
	primaryclass = {cs.CV},
	title = {{A Dual-Source Approach for 3D Pose Estimation from a Single Image}},
	url = {http://arxiv.org/abs/1509.06720v2; http://arxiv.org/pdf/1509.06720v2},
	x-fetchedfrom = {arXiv.org},
	year = {2016}
}

@misc{wandt19,
	abstract = {  This paper addresses the problem of 3D human pose estimation from single
images. While for a long time human skeletons were parameterized and fitted to
the observation by satisfying a reprojection error, nowadays researchers
directly use neural networks to infer the 3D pose from the observations.
However, most of these approaches ignore the fact that a reprojection
constraint has to be satisfied and are sensitive to overfitting. We tackle the
overfitting problem by ignoring 2D to 3D correspondences. This efficiently
avoids a simple memorization of the training data and allows for a weakly
supervised training. One part of the proposed reprojection network (RepNet)
learns a mapping from a distribution of 2D poses to a distribution of 3D poses
using an adversarial training approach. Another part of the network estimates
the camera. This allows for the definition of a network layer that performs the
reprojection of the estimated 3D pose back to 2D which results in a
reprojection loss function. Our experiments show that RepNet generalizes well
to unknown data and outperforms state-of-the-art methods when applied to unseen
data. Moreover, our implementation runs in real-time on a standard desktop PC.
},
	archiveprefix = {arXiv},
	author = {Wandt, Bastian and Rosenhahn, Bodo},
	comment = {published = 2019-02-26T11:23:54Z, updated = 2019-03-12T14:20:04Z, accepted to CVPR 2019},
	eprint = {1902.09868v2},
	localfile = {bibliography_resources/wandt - repnet weakly supervised training of an adversarioal network.pdf},
	month = mar,
	primaryclass = {cs.CV},
	title = {{RepNet: Weakly Supervised Training of an Adversarial Reprojection Network for 3D Human Pose Estimation}},
	url = {http://arxiv.org/abs/1902.09868v2; http://arxiv.org/pdf/1902.09868v2},
	x-fetchedfrom = {arXiv.org},
	year = {2019}
}

@misc{bogo16,
	abstract = {  We describe the first method to automatically estimate the 3D pose of the
human body as well as its 3D shape from a single unconstrained image. We
estimate a full 3D mesh and show that 2D joints alone carry a surprising amount
of information about body shape. The problem is challenging because of the
complexity of the human body, articulation, occlusion, clothing, lighting, and
the inherent ambiguity in inferring 3D from 2D. To solve this, we first use a
recently published CNN-based method, DeepCut, to predict (bottom-up) the 2D
body joint locations. We then fit (top-down) a recently published statistical
body shape model, called SMPL, to the 2D joints. We do so by minimizing an
objective function that penalizes the error between the projected 3D model
joints and detected 2D joints. Because SMPL captures correlations in human
shape across the population, we are able to robustly fit it to very little
data. We further leverage the 3D model to prevent solutions that cause
interpenetration. We evaluate our method, SMPLify, on the Leeds Sports,
HumanEva, and Human3.6M datasets, showing superior pose accuracy with respect
to the state of the art.
},
	archiveprefix = {arXiv},
	author = {Bogo, Federica and Kanazawa, Angjoo and Lassner, Christoph and Gehler, Peter and Romero, Javier and Black, Michael J.},
	comment = {published = 2016-07-27T14:46:36Z, updated = 2016-07-27T14:46:36Z, To appear in ECCV 2016},
	eprint = {1607.08128v1},
	localfile = {bibliography_resources/bogo - keep it smpl.pdf},
	month = jul,
	primaryclass = {cs.CV},
	title = {{Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image}},
	url = {http://arxiv.org/abs/1607.08128v1; http://arxiv.org/pdf/1607.08128v1},
	x-fetchedfrom = {arXiv.org},
	year = {2016}
}

@misc{moreno-noguer16,
	abstract = {  This paper addresses the problem of 3D human pose estimation from a single
image. We follow a standard two-step pipeline by first detecting the 2D
position of the $N$ body joints, and then using these observations to infer 3D
pose. For the first step, we use a recent CNN-based detector. For the second
step, most existing approaches perform 2$N$-to-3$N$ regression of the Cartesian
joint coordinates. We show that more precise pose estimates can be obtained by
representing both the 2D and 3D human poses using $N\times N$ distance
matrices, and formulating the problem as a 2D-to-3D distance matrix regression.
For learning such a regressor we leverage on simple Neural Network
architectures, which by construction, enforce positivity and symmetry of the
predicted matrices. The approach has also the advantage to naturally handle
missing observations and allowing to hypothesize the position of non-observed
joints. Quantitative results on Humaneva and Human3.6M datasets demonstrate
consistent performance gains over state-of-the-art. Qualitative evaluation on
the images in-the-wild of the LSP dataset, using the regressor learned on
Human3.6M, reveals very promising generalization results.
},
	archiveprefix = {arXiv},
	author = {Moreno-Noguer, Francesc},
	comment = {published = 2016-11-28T07:36:31Z, updated = 2016-11-28T07:36:31Z},
	eprint = {1611.09010v1},
	localfile = {bibliography_resources/moreno-noguer - 3d human pose estimation from a single image.pdf},
	month = nov,
	primaryclass = {cs.CV},
	title = {{3D Human Pose Estimation from a Single Image via Distance Matrix Regression}},
	url = {http://arxiv.org/abs/1611.09010v1; http://arxiv.org/pdf/1611.09010v1},
	x-fetchedfrom = {arXiv.org},
	year = {2016}
}

@article{kostrikov14,
	author = {Kostrikov, Ilya and Gall, J{\"u}ergen},
	doi = {10.5244/c.28.80},
	isbn = {1901725529},
	journal = {Proceedings of the British Machine Vision Conference 2014},
	localfile = {bibliography_resources/kostrikov - depth sweep regression forests.pdf},
	publisher = {British Machine Vision Association},
	title = {{Depth Sweep Regression Forests for Estimating 3D Human Pose from Images}},
	url = {http://dx.doi.org/10.5244/C.28.80},
	x-fetchedfrom = {DOI},
	year = {2014}
}

@misc{chen17,
	abstract = {  We explore 3D human pose estimation from a single RGB image. While many
approaches try to directly predict 3D pose from image measurements, we explore
a simple architecture that reasons through intermediate 2D pose predictions.
Our approach is based on two key observations (1) Deep neural nets have
revolutionized 2D pose estimation, producing accurate 2D predictions even for
poses with self occlusions. (2) Big-data sets of 3D mocap data are now readily
available, making it tempting to lift predicted 2D poses to 3D through simple
memorization (e.g., nearest neighbors). The resulting architecture is trivial
to implement with off-the-shelf 2D pose estimation systems and 3D mocap
libraries. Importantly, we demonstrate that such methods outperform almost all
state-of-the-art 3D pose estimation systems, most of which directly try to
regress 3D pose from 2D measurements.
},
	archiveprefix = {arXiv},
	author = {Chen, Ching-Hang and Ramanan, Deva},
	comment = {published = 2016-12-20T06:45:49Z, updated = 2017-04-11T07:33:51Z, Demo code: https://github.com/flyawaychase/3DHumanPose},
	eprint = {1612.06524v2},
	localfile = {bibliography_resources/chen - 3d = 2d + matching.pdf},
	month = apr,
	primaryclass = {cs.CV},
	title = {{3D Human Pose Estimation = 2D Pose Estimation + Matching}},
	url = {http://arxiv.org/abs/1612.06524v2; http://arxiv.org/pdf/1612.06524v2},
	x-fetchedfrom = {arXiv.org},
	year = {2017}
}

@article{gower75,
	author = {Gower, J. C.},
	doi = {10.1007/bf02291478},
	issn = {1860-0980},
	journal = {Psychometrika},
	localfile = {bibliography_resources/gower - generalized procrustes analysis.pdf},
	month = {Mar},
	number = {1},
	pages = {33--51},
	publisher = {Springer Nature},
	title = {{Generalized procrustes analysis}},
	url = {http://dx.doi.org/10.1007/BF02291478},
	volume = {40},
	x-fetchedfrom = {DOI},
	year = {1975}
}

@misc{tome17,
	abstract = {  We propose a unified formulation for the problem of 3D human pose estimation
from a single raw RGB image that reasons jointly about 2D joint estimation and
3D pose reconstruction to improve both tasks. We take an integrated approach
that fuses probabilistic knowledge of 3D human pose with a multi-stage CNN
architecture and uses the knowledge of plausible 3D landmark locations to
refine the search for better 2D locations. The entire process is trained
end-to-end, is extremely efficient and obtains state- of-the-art results on
Human3.6M outperforming previous approaches both on 2D and 3D errors.
},
	archiveprefix = {arXiv},
	author = {Tome, Denis and Russell, Chris and Agapito, Lourdes},
	comment = {published = 2017-01-01T22:50:51Z, updated = 2017-10-11T12:51:11Z, Paper presented at CVPR 17},
	doi = {10.1109/CVPR.2017.603},
	eprint = {1701.00295v4},
	localfile = {bibliography_resources/tome - lifting from the deep.pdf},
	month = oct,
	primaryclass = {cs.CV},
	title = {{Lifting from the Deep: Convolutional 3D Pose Estimation from a Single Image}},
	url = {http://arxiv.org/abs/1701.00295v4; http://arxiv.org/pdf/1701.00295v4},
	x-fetchedfrom = {arXiv.org},
	year = {2017}
}

@misc{rogez16,
	abstract = {  This paper addresses the problem of 3D human pose estimation in the wild. A
significant challenge is the lack of training data, i.e., 2D images of humans
annotated with 3D poses. Such data is necessary to train state-of-the-art CNN
architectures. Here, we propose a solution to generate a large set of
photorealistic synthetic images of humans with 3D pose annotations. We
introduce an image-based synthesis engine that artificially augments a dataset
of real images with 2D human pose annotations using 3D Motion Capture (MoCap)
data. Given a candidate 3D pose our algorithm selects for each joint an image
whose 2D pose locally matches the projected 3D pose. The selected images are
then combined to generate a new synthetic image by stitching local image
patches in a kinematically constrained manner. The resulting images are used to
train an end-to-end CNN for full-body 3D pose estimation. We cluster the
training data into a large number of pose classes and tackle pose estimation as
a K-way classification problem. Such an approach is viable only with large
training sets such as ours. Our method outperforms the state of the art in
terms of 3D pose estimation in controlled environments (Human3.6M) and shows
promising results for in-the-wild images (LSP). This demonstrates that CNNs
trained on artificial images generalize well to real images.
},
	archiveprefix = {arXiv},
	author = {Rogez, Gr{\'e}gory and Schmid, Cordelia},
	comment = {published = 2016-07-07T15:30:05Z, updated = 2016-10-28T12:43:51Z, 9 pages, accepted to appear in NIPS 2016},
	eprint = {1607.02046v2},
	localfile = {bibliography_resources/rogez - mocap-guided data augmentation.pdf},
	month = oct,
	primaryclass = {cs.CV},
	title = {{MoCap-guided Data Augmentation for 3D Pose Estimation in the Wild}},
	url = {http://arxiv.org/abs/1607.02046v2; http://arxiv.org/pdf/1607.02046v2},
	x-fetchedfrom = {arXiv.org},
	year = {2016}
}

@misc{zhou18,
	abstract = {  Recovering 3D full-body human pose is a challenging problem with many
applications. It has been successfully addressed by motion capture systems with
body worn markers and multiple cameras. In this paper, we address the more
challenging case of not only using a single camera but also not leveraging
markers: going directly from 2D appearance to 3D geometry. Deep learning
approaches have shown remarkable abilities to discriminatively learn 2D
appearance features. The missing piece is how to integrate 2D, 3D and temporal
information to recover 3D geometry and account for the uncertainties arising
from the discriminative model. We introduce a novel approach that treats 2D
joint locations as latent variables whose uncertainty distributions are given
by a deep fully convolutional neural network. The unknown 3D poses are modeled
by a sparse representation and the 3D parameter estimates are realized via an
Expectation-Maximization algorithm, where it is shown that the 2D joint
location uncertainties can be conveniently marginalized out during inference.
Extensive evaluation on benchmark datasets shows that the proposed approach
achieves greater accuracy over state-of-the-art baselines. Notably, the
proposed approach does not require synchronized 2D-3D data for training and is
applicable to "in-the-wild" images, which is demonstrated with the MPII
dataset.
},
	archiveprefix = {arXiv},
	author = {Zhou, Xiaowei and Zhu, Menglong and Pavlakos, Georgios and Leonardos, Spyridon and Derpanis, Kostantinos G. and Daniilidis, Kostas},
	comment = {published = 2017-01-09T21:25:06Z, updated = 2018-03-09T09:28:11Z, Accepted by PAMI. Extended version of the following paper: Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video. X Zhou, M Zhu, S Leonardos, K Derpanis, K Daniilidis. CVPR 2016. arXiv admin note: substantial text overlap with arXiv:1511.09439},
	eprint = {1701.02354v2},
	localfile = {bibliography_resources/zhou - monocap.pdf},
	month = mar,
	primaryclass = {cs.CV},
	title = {{MonoCap: Monocular Human Motion Capture using a CNN Coupled with a Geometric Prior}},
	url = {http://arxiv.org/abs/1701.02354v2; http://arxiv.org/pdf/1701.02354v2},
	x-fetchedfrom = {arXiv.org},
	year = {2018}
}

@misc{zhou16_2,
	abstract = {  Learning articulated object pose is inherently difficult because the pose is
high dimensional but has many structural constraints. Most existing work do not
model such constraints and does not guarantee the geometric validity of their
pose estimation, therefore requiring a post-processing to recover the correct
geometry if desired, which is cumbersome and sub-optimal. In this work, we
propose to directly embed a kinematic object model into the deep neutral
network learning for general articulated object pose estimation. The kinematic
function is defined on the appropriately parameterized object motion variables.
It is differentiable and can be used in the gradient descent based optimization
in network training. The prior knowledge on the object geometric model is fully
exploited and the structure is guaranteed to be valid. We show convincing
experiment results on a toy example and the 3D human pose estimation problem.
For the latter we achieve state-of-the-art result on Human3.6M dataset.
},
	archiveprefix = {arXiv},
	author = {Zhou, Xingyi and Sun, Xiao and Zhang, Wei and Liang, Shuang and Wei, Yichen},
	comment = {published = 2016-09-17T11:22:11Z, updated = 2016-09-17T11:22:11Z, ECCV Workshop on Geometry Meets Deep Learning, 2016},
	eprint = {1609.05317v1},
	localfile = {bibliography_resources/zhou - deep kinematic pose regression.pdf},
	month = sep,
	primaryclass = {cs.CV},
	title = {{Deep Kinematic Pose Regression}},
	url = {http://arxiv.org/pdf/1609.05317v1; http://arxiv.org/abs/1609.05317v1},
	x-fetchedfrom = {arXiv.org},
	year = {2016}
}

@misc{tekin16,
	abstract = {  We propose an efficient approach to exploiting motion information from
consecutive frames of a video sequence to recover the 3D pose of people.
Previous approaches typically compute candidate poses in individual frames and
then link them in a post-processing step to resolve ambiguities. By contrast,
we directly regress from a spatio-temporal volume of bounding boxes to a 3D
pose in the central frame.
  We further show that, for this approach to achieve its full potential, it is
essential to compensate for the motion in consecutive frames so that the
subject remains centered. This then allows us to effectively overcome
ambiguities and improve upon the state-of-the-art by a large margin on the
Human3.6m, HumanEva, and KTH Multiview Football 3D human pose estimation
benchmarks.
},
	archiveprefix = {arXiv},
	author = {Tekin, Bugra and Rozantsev, Artem and Lepetit, Vincent and Fua, Pascal},
	comment = {published = 2015-11-20T17:08:18Z, updated = 2016-09-02T09:38:08Z, Published in CVPR 2016. supersedes arXiv:1504.08200},
	eprint = {1511.06692v4},
	localfile = {bibliography_resources/tekin - direct prediction of 3d body poses.pdf},
	month = sep,
	primaryclass = {cs.CV},
	title = {{Direct Prediction of 3D Body Poses from Motion Compensated Sequences}},
	url = {http://arxiv.org/abs/1511.06692v4; http://arxiv.org/pdf/1511.06692v4},
	x-fetchedfrom = {arXiv.org},
	year = {2016}
}

@misc{jahangiri17,
	abstract = {  We propose a method to generate multiple diverse and valid human pose
hypotheses in 3D all consistent with the 2D detection of joints in a monocular
RGB image. We use a novel generative model uniform (unbiased) in the space of
anatomically plausible 3D poses. Our model is compositional (produces a pose by
combining parts) and since it is restricted only by anatomical constraints it
can generalize to every plausible human 3D pose. Removing the model bias
intrinsically helps to generate more diverse 3D pose hypotheses. We argue that
generating multiple pose hypotheses is more reasonable than generating only a
single 3D pose based on the 2D joint detection given the depth ambiguity and
the uncertainty due to occlusion and imperfect 2D joint detection. We hope that
the idea of generating multiple consistent pose hypotheses can give rise to a
new line of future work that has not received much attention in the literature.
We used the Human3.6M dataset for empirical evaluation.
},
	archiveprefix = {arXiv},
	author = {Jahangiri, Ehsan and Yuille, Alan L.},
	comment = {published = 2017-02-08T02:54:25Z, updated = 2017-08-20T19:39:19Z, accepted to ICCV 2017 (PeopleCap)},
	eprint = {1702.02258v2},
	localfile = {bibliography_resources/jahangiri - generating multiple diverse hypotheses for hume 3d pose.pdf},
	month = aug,
	primaryclass = {cs.CV},
	title = {{Generating Multiple Diverse Hypotheses for Human 3D Pose Consistent with 2D Joint Detections}},
	url = {http://arxiv.org/abs/1702.02258v2; http://arxiv.org/pdf/1702.02258v2},
	x-fetchedfrom = {arXiv.org},
	year = {2017}
}

@misc{mehta17,
	abstract = {  We propose a CNN-based approach for 3D human body pose estimation from single
RGB images that addresses the issue of limited generalizability of models
trained solely on the starkly limited publicly available 3D pose data. Using
only the existing 3D pose data and 2D pose data, we show state-of-the-art
performance on established benchmarks through transfer of learned features,
while also generalizing to in-the-wild scenes. We further introduce a new
training set for human body pose estimation from monocular images of real
humans that has the ground truth captured with a multi-camera marker-less
motion capture system. It complements existing corpora with greater diversity
in pose, human appearance, clothing, occlusion, and viewpoints, and enables an
increased scope of augmentation. We also contribute a new benchmark that covers
outdoor and indoor scenes, and demonstrate that our 3D pose dataset shows
better in-the-wild performance than existing annotated data, which is further
improved in conjunction with transfer learning from 2D pose data. All in all,
we argue that the use of transfer learning of representations in tandem with
algorithmic and data contributions is crucial for general 3D body pose
estimation.
},
	archiveprefix = {arXiv},
	author = {Mehta, Dushyant and Rhodin, Helge and Casas, Dan and Fua, Pascal and Sotnychenko, Oleksandr and Xu, Weipeng and Theobalt, Christian},
	comment = {published = 2016-11-29T20:03:19Z, updated = 2017-10-04T15:21:46Z, Accepted at the International Conference on 3D Vision (3DV) 2017},
	eprint = {1611.09813v5},
	localfile = {bibliography_resources/mehta - 3d pose estimation using improved cnn supervision.pdf},
	month = oct,
	primaryclass = {cs.CV},
	title = {{Monocular 3D Human Pose Estimation In The Wild Using Improved CNN Supervision}},
	url = {http://arxiv.org/abs/1611.09813v5; http://arxiv.org/pdf/1611.09813v5},
	x-fetchedfrom = {arXiv.org},
	year = {2017}
}

@misc{pavlakos17,
	abstract = {  This paper addresses the challenge of 3D human pose estimation from a single
color image. Despite the general success of the end-to-end learning paradigm,
top performing approaches employ a two-step solution consisting of a
Convolutional Network (ConvNet) for 2D joint localization and a subsequent
optimization step to recover 3D pose. In this paper, we identify the
representation of 3D pose as a critical issue with current ConvNet approaches
and make two important contributions towards validating the value of end-to-end
learning for this task. First, we propose a fine discretization of the 3D space
around the subject and train a ConvNet to predict per voxel likelihoods for
each joint. This creates a natural representation for 3D pose and greatly
improves performance over the direct regression of joint coordinates. Second,
to further improve upon initial estimates, we employ a coarse-to-fine
prediction scheme. This step addresses the large dimensionality increase and
enables iterative refinement and repeated processing of the image features. The
proposed approach outperforms all state-of-the-art methods on standard
benchmarks achieving a relative error reduction greater than 30\% on average.
Additionally, we investigate using our volumetric representation in a related
architecture which is suboptimal compared to our end-to-end approach, but is of
practical interest, since it enables training when no image with corresponding
3D groundtruth is available, and allows us to present compelling results for
in-the-wild images.
},
	archiveprefix = {arXiv},
	author = {Pavlakos, Georgios and Zhou, Xiaowei and Derpanis, Konstantinos G. and Daniilidis, Kostas},
	comment = {published = 2016-11-23T15:06:18Z, updated = 2017-07-26T12:10:16Z, CVPR 2017 Camera Ready. Project Page: https://www.seas.upenn.edu/~pavlakos/projects/volumetric/},
	eprint = {1611.07828v2},
	localfile = {bibliography_resources/pavlakos - coarse-to-fine volumetric prediction.pdf},
	month = jul,
	primaryclass = {cs.CV},
	title = {{Coarse-to-Fine Volumetric Prediction for Single-Image 3D Human Pose}},
	url = {http://arxiv.org/abs/1611.07828v2; http://arxiv.org/pdf/1611.07828v2},
	x-fetchedfrom = {arXiv.org},
	year = {2017}
}

@misc{goodfellow17,
	abstract = {  This report summarizes the tutorial presented by the author at NIPS 2016 on
generative adversarial networks (GANs). The tutorial describes: (1) Why
generative modeling is a topic worth studying, (2) how generative models work,
and how GANs compare to other generative models, (3) the details of how GANs
work, (4) research frontiers in GANs, and (5) state-of-the-art image models
that combine GANs with other methods. Finally, the tutorial contains three
exercises for readers to complete, and the solutions to these exercises.
},
	archiveprefix = {arXiv},
	author = {Goodfellow, Ian},
	comment = {published = 2016-12-31T19:17:17Z, updated = 2017-04-03T21:57:48Z, v2-v4 are all typo fixes. No substantive changes relative to v1},
	eprint = {1701.00160v4},
	localfile = {bibliography_resources/goodfellow - nips 2016 tutorial: GAN.pdf},
	month = apr,
	primaryclass = {cs.LG},
	title = {{NIPS 2016 Tutorial: Generative Adversarial Networks}},
	url = {http://arxiv.org/abs/1701.00160v4; http://arxiv.org/pdf/1701.00160v4},
	x-fetchedfrom = {arXiv.org},
	year = {2017}
}

@misc{gidel19,
	abstract = {  Games generalize the single-objective optimization paradigm by introducing
different objective functions for different players. Differentiable games often
proceed by simultaneous or alternating gradient updates. In machine learning,
games are gaining new importance through formulations like generative
adversarial networks (GANs) and actor-critic systems. However, compared to
single-objective optimization, game dynamics are more complex and less
understood. In this paper, we analyze gradient-based methods with momentum on
simple games. We prove that alternating updates are more stable than
simultaneous updates. Next, we show both theoretically and empirically that
alternating gradient updates with a negative momentum term achieves convergence
in a difficult toy adversarial problem, but also on the notoriously difficult
to train saturating GANs.
},
	archiveprefix = {arXiv},
	author = {Gidel, Gauthier and Hemmat, Reyhane Askari and Pezeshki, Mohammad and Lepriol, Remi and Huang, Gabriel and Lacoste-Julien, Simon and Mitliagkas, Ioannis},
	comment = {published = 2018-07-12T17:46:56Z, updated = 2019-03-27T16:32:10Z, Appears in: Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics (AISTATS 2019). 25 pages},
	eprint = {1807.04740v3},
	localfile = {bibliography_resources/gidel - negative momentum.pdf},
	month = mar,
	primaryclass = {cs.LG},
	title = {{Negative Momentum for Improved Game Dynamics}},
	url = {http://arxiv.org/abs/1807.04740v3; http://arxiv.org/pdf/1807.04740v3},
	x-fetchedfrom = {arXiv.org},
	year = {2019}
}

@misc{pavllo19,
	abstract = {  In this work, we demonstrate that 3D poses in video can be effectively
estimated with a fully convolutional model based on dilated temporal
convolutions over 2D keypoints. We also introduce back-projection, a simple and
effective semi-supervised training method that leverages unlabeled video data.
We start with predicted 2D keypoints for unlabeled video, then estimate 3D
poses and finally back-project to the input 2D keypoints. In the supervised
setting, our fully-convolutional model outperforms the previous best result
from the literature by 6 mm mean per-joint position error on Human3.6M,
corresponding to an error reduction of 11\%, and the model also shows
significant improvements on HumanEva-I. Moreover, experiments with
back-projection show that it comfortably outperforms previous state-of-the-art
results in semi-supervised settings where labeled data is scarce. Code and
models are available at https://github.com/facebookresearch/VideoPose3D
},
	archiveprefix = {arXiv},
	author = {Pavllo, Dario and Feichtenhofer, Christoph and Grangier, David and Auli, Michael},
	comment = {published = 2018-11-28T18:56:36Z, updated = 2019-03-29T13:36:46Z, CVPR 2019},
	eprint = {1811.11742v2},
	localfile = {bibliography_resources/pavllo - 3d human pose estimation in video with temporal convolutions},
	month = mar,
	primaryclass = {cs.CV},
	title = {{3D human pose estimation in video with temporal convolutions and semi-supervised training}},
	url = {http://arxiv.org/abs/1811.11742v2; http://arxiv.org/pdf/1811.11742v2},
	x-fetchedfrom = {arXiv.org},
	year = {2019}
}

@misc{chorowski14,
	abstract = {  We replace the Hidden Markov Model (HMM) which is traditionally used in in
continuous speech recognition with a bi-directional recurrent neural network
encoder coupled to a recurrent neural network decoder that directly emits a
stream of phonemes. The alignment between the input and output sequences is
established using an attention mechanism: the decoder emits each symbol based
on a context created with a subset of input symbols elected by the attention
mechanism. We report initial results demonstrating that this new approach
achieves phoneme error rates that are comparable to the state-of-the-art
HMM-based decoders, on the TIMIT dataset.
},
	archiveprefix = {arXiv},
	author = {Chorowski, Jan and Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	comment = {published = 2014-12-04T10:00:19Z, updated = 2014-12-04T10:00:19Z, As accepted to: Deep Learning and Representation Learning Workshop, NIPS 2014},
	eprint = {1412.1602v1},
	localfile = {bibliography_resources/chorowski - e2e continuous speech recognition.pdf},
	month = dec,
	primaryclass = {cs.NE},
	title = {{End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results}},
	url = {http://arxiv.org/abs/1412.1602v1; http://arxiv.org/pdf/1412.1602v1},
	x-fetchedfrom = {arXiv.org},
	year = {2014}
}

@article{tung17,
	abstract = {  Researchers have developed excellent feed-forward models that learn to map
images to desired outputs, such as to the images' latent factors, or to other
images, using supervised learning. Learning such mappings from unlabelled data,
or improving upon supervised models by exploiting unlabelled data, remains
elusive. We argue that there are two important parts to learning without
annotations: (i) matching the predictions to the input observations, and (ii)
matching the predictions to known priors. We propose Adversarial Inverse
Graphics networks (AIGNs): weakly supervised neural network models that combine
feedback from rendering their predictions, with distribution matching between
their predictions and a collection of ground-truth factors. We apply AIGNs to
3D human pose estimation and 3D structure and egomotion estimation, and
outperform models supervised by only paired annotations. We further apply AIGNs
to facial image transformation using super-resolution and inpainting renderers,
while deliberately adding biases in the ground-truth datasets. Our model
seamlessly incorporates such biases, rendering input faces towards young, old,
feminine, masculine or Tom Cruise-like equivalents (depending on the chosen
bias), or adding lip and nose augmentations while inpainting concealed lips and
noses.
},
	archiveprefix = {arXiv},
	author = {Tung, Hsiao-Yu Fish and Harley, Adam W. and Seto, William and Fragkiadaki, Katerina},
	comment = {published = 2017-05-31T16:30:07Z, updated = 2017-09-02T01:10:17Z},
	eprint = {1705.11166v3},
	localfile = {bibliography_resources/tung - adversarial inverse graphics networks.pdf},
	month = sep,
	pages = {The IEEE International Conference on Computer Vision },
	primaryclass = {cs.CV},
	title = {{Adversarial Inverse Graphics Networks: Learning 2D-to-3D Lifting and Image-to-Image Translation from Unpaired Supervision}},
	url = {http://arxiv.org/abs/1705.11166v3; http://arxiv.org/pdf/1705.11166v3},
	x-fetchedfrom = {arXiv.org},
	year = {2017}
}

@misc{kingma17,
	abstract = {  We introduce Adam, an algorithm for first-order gradient-based optimization
of stochastic objective functions, based on adaptive estimates of lower-order
moments. The method is straightforward to implement, is computationally
efficient, has little memory requirements, is invariant to diagonal rescaling
of the gradients, and is well suited for problems that are large in terms of
data and/or parameters. The method is also appropriate for non-stationary
objectives and problems with very noisy and/or sparse gradients. The
hyper-parameters have intuitive interpretations and typically require little
tuning. Some connections to related algorithms, on which Adam was inspired, are
discussed. We also analyze the theoretical convergence properties of the
algorithm and provide a regret bound on the convergence rate that is comparable
to the best known results under the online convex optimization framework.
Empirical results demonstrate that Adam works well in practice and compares
favorably to other stochastic optimization methods. Finally, we discuss AdaMax,
a variant of Adam based on the infinity norm.
},
	archiveprefix = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	comment = {published = 2014-12-22T13:54:29Z, updated = 2017-01-30T01:27:54Z, Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
	eprint = {1412.6980v9},
	month = jan,
	primaryclass = {cs.LG},
	title = {{Adam: A Method for Stochastic Optimization}},
	url = {http://arxiv.org/abs/1412.6980v9; http://arxiv.org/pdf/1412.6980v9},
	x-fetchedfrom = {arXiv.org},
	year = {2017}
}

@inproceedings{trumble17,
	author = {Trumble, Matt and Gilbert, Andrew and Malleson, Charles and Hilton, Adrian and Collomosse, John},
	booktitle = {{2017 British Machine Vision Conference (BMVC)}},
	title = {{Total Capture: 3D Human Pose Estimation Fusing Video and Inertial Sensors}},
	year = {2017}
}

@misc{grinciunaite16,
	abstract = {  This paper explores the capabilities of convolutional neural networks to deal
with a task that is easily manageable for humans: perceiving 3D pose of a human
body from varying angles. However, in our approach, we are restricted to using
a monocular vision system. For this purpose, we apply a convolutional neural
network approach on RGB videos and extend it to three dimensional convolutions.
This is done via encoding the time dimension in videos as the 3\ts{rd}
dimension in convolutional space, and directly regressing to human body joint
positions in 3D coordinate space. This research shows the ability of such a
network to achieve state-of-the-art performance on the selected Human3.6M
dataset, thus demonstrating the possibility of successfully representing
temporal data with an additional dimension in the convolutional operation.
},
	archiveprefix = {arXiv},
	author = {Grinciunaite, Agne and Gudi, Amogh and Tasli, Emrah and den Uyl, Marten},
	comment = {published = 2016-08-31T20:55:26Z, updated = 2016-10-19T12:44:15Z, Accepted at ECCV 2016 Workshop on: Brave new ideas for motion representations in videos},
	doi = {10.1007/978-3-319-49409-8_5},
	eprint = {1609.00036v3},
	localfile = {bibliography_resources/grinciunaite16 - human pose estimation in space and time.pdf},
	month = oct,
	primaryclass = {cs.CV},
	title = {{Human Pose Estimation in Space and Time using 3D CNN}},
	url = {http://arxiv.org/abs/1609.00036v3; http://arxiv.org/pdf/1609.00036v3},
	x-fetchedfrom = {arXiv.org},
	year = {2016}
}

@misc{andriluka18,
	abstract = {  Human poses and motions are important cues for analysis of videos with people
and there is strong evidence that representations based on body pose are highly
effective for a variety of tasks such as activity recognition, content
retrieval and social signal processing. In this work, we aim to further advance
the state of the art by establishing "PoseTrack", a new large-scale benchmark
for video-based human pose estimation and articulated tracking, and bringing
together the community of researchers working on visual human analysis. The
benchmark encompasses three competition tracks focusing on i) single-frame
multi-person pose estimation, ii) multi-person pose estimation in videos, and
iii) multi-person articulated tracking. To facilitate the benchmark and
challenge we collect, annotate and release a new \%large-scale benchmark dataset
that features videos with multiple people labeled with person tracks and
articulated pose. A centralized evaluation server is provided to allow
participants to evaluate on a held-out test set. We envision that the proposed
benchmark will stimulate productive research both by providing a large and
representative training dataset as well as providing a platform to objectively
evaluate and compare the proposed methods. The benchmark is freely accessible
at https://posetrack.net.
},
	archiveprefix = {arXiv},
	author = {Andriluka, Mykhaylo and Iqbal, Umar and Insafutdinov, Eldar and Pishchulin, Leonid and Milan, Anton and Gall, Juergen and Schiele, Bernt},
	comment = {published = 2017-10-27T06:20:30Z, updated = 2018-04-10T18:20:56Z, www.posetrack.net},
	eprint = {1710.10000v2},
	month = apr,
	primaryclass = {cs.CV},
	title = {{PoseTrack: A Benchmark for Human Pose Estimation and Tracking}},
	url = {http://arxiv.org/abs/1710.10000v2; http://arxiv.org/pdf/1710.10000v2},
	x-fetchedfrom = {arXiv.org},
	year = {2018}
}

@misc{guler18,
	abstract = {  In this work, we establish dense correspondences between RGB image and a
surface-based representation of the human body, a task we refer to as dense
human pose estimation. We first gather dense correspondences for 50K persons
appearing in the COCO dataset by introducing an efficient annotation pipeline.
We then use our dataset to train CNN-based systems that deliver dense
correspondence 'in the wild', namely in the presence of background, occlusions
and scale variations. We improve our training set's effectiveness by training
an 'inpainting' network that can fill in missing groundtruth values and report
clear improvements with respect to the best results that would be achievable in
the past. We experiment with fully-convolutional networks and region-based
models and observe a superiority of the latter; we further improve accuracy
through cascading, obtaining a system that delivers highly0accurate results in
real time. Supplementary materials and videos are provided on the project page
http://densepose.org
},
	archiveprefix = {arXiv},
	author = {G{\"u}ler, R{\i}za Alp and Neverova, Natalia and Kokkinos, Iasonas},
	comment = {published = 2018-02-01T18:53:26Z, updated = 2018-02-01T18:53:26Z},
	eprint = {1802.00434v1},
	month = feb,
	primaryclass = {cs.CV},
	title = {{DensePose: Dense Human Pose Estimation In The Wild}},
	url = {http://arxiv.org/abs/1802.00434v1; http://arxiv.org/pdf/1802.00434v1},
	x-fetchedfrom = {arXiv.org},
	year = {2018}
}

