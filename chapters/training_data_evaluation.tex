\section{Evaluation on Different Datasets}
\label{sec:evaluation}

In this section the model's performance will be analyzed for different configurations of training and test data.
For training the Adam Optimizer \cite{kingma17} with an initial learning rate of $0.0002$ and $\beta_1 = 0.5$ is used for both generator and discriminator.
This particular choice of $\beta_1$ has shown to result in faster convergence.
As in the work by \citet{drover18}, the data is split up into batches of 32768 poses for training.
Generator and discriminator are trained alternately with the same batch of poses.
Before adjusting the networks' parameters, adaptive clipping is applied to the gradients \cite[Section~3.2.1]{chorowski14}.


\subsection{Results for Synthetic Data}

Later in this work, the effects of pose normalization will be analyzed.
For a meaningful comparison of normalized and non-normalized poses, in a first step the results for poses which are naturally normalized are obtained.
The normalization applied to the 2D poses consists of two parts: Centering the pose node and scaling.
As will further be elaborated in Section \ref{sec:effects-of-normalization}, the need for scaling can hardly be avoided.
Centering however can be easily avoided, when the camera photographing the 3D poses is centered at the pose's root joint.

For a fine control of the 2D poses' degree of normalization the 2D poses included in the dataset are not viable.
Hence, 2D poses are synthetically generated by projecting the monocular 3D poses available in the Human3.6M dataset.
These synthetic poses are created by "photographing" them with virtual cameras that provide the desired 2D poses.
For training and testing, cameras with integer azimuth and elevation angles randomly sampled from [0, 359] degrees and [0, 20] degrees are used.
This choice of elevation angles is based on the heuristic that in reality people are rather photographed from above than from below.
The 3D poses are first normalized such that the norm limb has length 1 and then photographed with a camera-root-joint distance of 10 units.
Thus and from the specific choice of elevation angles, in most cases the projected root limb has a length of almost 0.1 and only slight scaling is necessary.
For training, \citet{drover18} follow a similar procedure and augment synthetic 2D poses, although they use 8 fixed cameras instead of randomly sampling camera angles for each pose.

\input{figures/original_system_protocol1_table}
\input{figures/original_system_protocol2_table}
\input{figures/original_system_total_capture_table}

Table \ref{tbl:results-original-protocol1} shows the results for \textbf{Protocol 1} reported in \cite{drover18} and for the replicated system trained with synthetic data.
For the evaluation of the latter, one synthetic 2D poses is deterministically sampled from each monocular 3D pose available for subject 11.
The results for the synthetic training data can not compete with the results presented by \citet{drover18}.
The exact source of the 6.8mm average discrepancy is not clear; the reasons can only be speculated about.
\citet{drover18} don't mention whether they also use synthetic poses for testing or evaluate only on 2D poses provided in Human3.6M.
Before calculating the MPJPE, they apply a similarity transformation, but don't explicitly mention the Procrustes Analysis used in this work.
Further discrepancy can also arise from different internal configurations, like the exact training procedure or the applied gradient clipping.
Finally, extensive mining for the best performing model might also be a reason for the lower MPJPE.

Evaluation results for the monocular 2D poses in Human3.6M are also displayed in Table \ref{tbl:results-original-protocol1}.
The MPJPEs for the different categories are slightly worse than the ones for the synthetic data.
This comes as no surprise, as most machine learning systems will perform better on test data which is more similar to the training data.
For the synthetic testing data, this is certainly the case.

\Todo{Add correct difference}
The results for \textbf{Protocol 2} are given in Table \ref{tbl:results-original-protocol2}.
As rotation is not allowed for aligning the poses, the MPJPEs are approximately 88.8mm worse than those for Protocol 1.
Overall, the same relations between the synthetic poses and the poses from Human3.6M can be seen.
As \citet{drover18} also allow rigid alignment for Protocol 2 their results are not directly comparable.

On the test data of the TotalCapture dataset, the average MPJPE for Protocol 1 is 68.3mm and for Protocol 2 100.2mm respectively (Table \ref{tbl:results-original-totalcapture}).
Apart from the aforementioned phenomenon, the key points of the joints are slightly different from the ones in Human3.6M.
Especially the angle between the hips is noticeable.
In Human3.6M, this angle is 180 degrees and the spine is perpendicular to the hips, whereas in TotalCapture the angle is substantially smaller (see Figure \ref{fig:human-totalcapture}).
Although the results are slightly worse for TotalCapture, they still show that the system generalizes to unseen data in a reasonable way.

\input{figures/human-totalcapture-figure}
\Todo{Add a few qualitative results.}


\subsection{Training with Augmented and Real Data}
The results for the test data of the Human3.6M dataset in Tables \ref{tbl:results-original-protocol1} and \ref{tbl:results-original-protocol2} are not quite satisfying, as they are clearly worse then those for the synthetic data.
Ideally, the system performs equally good on all kinds of data.
The reasons for the worse results may be of different nature:
As mentioned above, machine learning systems tend to perform better on data similar to the training data.
Another reason might be that Human3.6M's monocular 2D poses are in fact being normalized, both in scale and position.

In order to figure out why the MPJPE is higher, the monocular 2D poses are added to the training set.
The generator receives both real and synthetic poses at a 1:1 ratio.
As described in Section \ref{sec:network}, during training the generator tries to learn the real data distribution.
In order to have a chance to achieve this, it must be able to generate poses similar to the real poses. 
This comes down to the Random Projection Layer, where the poses are projected with camera distance $d$ and a root joint offset of $0$ (which means the camera is centered at the root joint).
2D poses projected with a camera distance other than $d$ or with a non-zero root joint offset are inherently different.
This will be elaborate on in Section \ref{sec:effects-of-normalization}.
Thus there are two options:
Projecting the generated 3D poses with a camera distance and offset similar to the real data or feeding only data to the discriminator that resembles the generator's output.
As the real camera distance might not be known in general, and offsets would still need to be sampled, option 2 was chosen.
Thus the discriminator still only receives synthetic poses.

\input{figures/real_and_synthetic_protocol_2_table}

Table \ref{tbl:results-real-and-synthetic-protocol2} shows the results for both synthetic data and the monocular 2D test poses from Human3.6M.
\Todo[inline]{Add description of results when available}

One the one hand, these results are positive regarding practical use:
When the system is trained with real world data from a specific application, it will perform better in that application.
On the other hand, because the results are still not competitive with those for synthetic data, 
Important: Generator should output the same "kind" of data as the "real" data the discriminator receives.
Improvements of evaluation on real 2D data. 
This shows that the system is expected to perform well in scenarios where it is trained with data it should later infer.
Results for original data get better

\Todo{
	"The reasons for the additional error of the real 2D data (shifting) will be extensively discussed in Section \ref{sec:effects-of-normalization}}

\subsection{Training with augmented cameras similar to the real cameras}
Performance of system with cameras similar to the real cameras.
Results for real 2D data.