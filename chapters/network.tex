\section{Network architecture}\label{sec:network}
\Todo[inline]{Mention root joint}
\begin{itemize}
	\item basic principle of gans
	\item Basic architecture used in the paper
	\item Mention batch norm not working with reference to other paper and communication with authors
	\item Projection and reprojection.
	\item Norm limb normalization
\end{itemize}

Assumptions:
\begin{enumerate}
	\item The camera is centered at the root joint.
	\item The distance between the camera is such that the length of the projected norm limb is $0.1$. 
\end{enumerate}
\begin{itemize}
	\item[(1)] All 2d data can be scaled in a way that a norm limb is of length 0.1. 
	Problems with correct camera distance, see section \ref{sec:z-shift-error}. 
	Problem when camera is perpendicular to the norm limb.
	\item[(2)] All data was captured with the same camera distance.
\end{itemize}

For the projection and re-projection of poses a pinhole camera model is used.
It has a focal length $f$ and is described by
\begin{equation}
	p = 
	\begin{pmatrix}
	x\\
	y
	\end{pmatrix}
	= \frac{f}{Z} \cdot 	
	\begin{pmatrix}
	X\\
	Y
	\end{pmatrix} \ ,
\end{equation}
where $P = (X, Y, Z)$ is a point in camera coordinates and p the projected point on the (virtual) image plane.
Throughout this work coordinates of 3D points will be denoted by capital letters and 2D points by lower case letters.

In general, the \textit{generator} $G$ is a neural network that outputs \unsure{wording: elements or values?} elements from the distribution $P_{fake}$.
In the application of 3D human pose estimation the generator receives a projection of a 3D pose as input and outputs the corresponding 3D pose.
More precisely the generator only produces a depth offset $o_i$ for each joint in the input.
The entire pose can then be calculated by first calculating absolute depths:
\begin{equation}
	Z_i = \max \{f, Z + o_i\} \ .
\end{equation}
The clipping makes sure the points are re-projected in front of the camera.
\Todo{During training: $Z = 10$ and $f = 1$}
Afterwards the 2D points are re-projected along the projection rays:
\begin{equation}
	\begin{pmatrix}
	X_i\\
	Y_i
	\end{pmatrix} = \frac{Z_i}{f} \cdot
	\begin{pmatrix}
	x_i\\
	y_i
	\end{pmatrix}
\end{equation}
The 3D pose is then given by $\{(X_i, Y_i, Z_i)~|~ 1 \leq i \leq n\}$.

In production the generation process would be over at this point but for training, 2D poses have to be sampled from the reprojected 3D pose.
The 3D poses are aligned with the origin and randomly rotated with azimuth angles in $[0, 359]$ and elevation $[0, 19]$.
\Todo{When 2D poses are sampled, the extrinsic parameters of the synthetic camera model should match those of the real camera(s)}

The \textit{discriminator} $D$ is defined as a neural network that consumes real 2D poses from the distribution $P_{real}$ and fake 2D poses drawn from $P_{fake}$ and outputs a scalar value in $[0, 1]$.
A GAN can be formulated as as a minimax game with a value function $V(D, G)$ as follows \cite{goodfellow14}: 
\begin{equation}
	\min_G \max_D V(D, G) = \mathbb{E}_{r\sim P_{real}}(\log(D(r))) + \mathbb{E}_{p\sim P_{fake}}(\log(1 - D(p)))
\end{equation}

De facto loss functions for generator and discriminator:
\begin{align}
	loss_G &= \mathbb{E}_{p\sim P_{fake}}(\log(1 - D(p))) \\
	loss_D &= \mathbb{E}_{r\sim P_{real}}(\log(D(r))) + \mathbb{E}_{p\sim P_{fake}}(\log(1 - D(p)))
\end{align} 