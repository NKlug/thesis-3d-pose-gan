\section{Network architecture}\label{sec:network}
\Todo[inline]{Mention root joint}
\begin{itemize}
	\item Mention batch norm not working with reference to other paper and communication with authors
	\item Norm limb normalization
	\item Heuristic: If a random projection of a 3D pose is realistic, the 3D pose is also realistic
	\item mention residuals: $dx$ and $dy$ at normalization
\end{itemize}

In their work "Can 3D Pose Be Learned from 2D Projections Alone?" \citet{drover18} propose a 3D human pose estimation system that uses a Generative Adversarial Network.

\subsection{Generative Adversarial Networks}
Generative Adversarial Networks (GANs) have first been presented by \citet{goodfellow14} in 2014.
As the name suggests, they are generative models involving two adversary agents, a \emph{generator} and a \emph{discriminator}.
The discriminator $D_{\theta_D}$ with internal parameters $\theta_D$ tries to determine whether its input belongs to the real data distribution $p_{real}$ (the distribution to be learned) or to the distribution $p_{fake}$, which is implicitly captured by the generator.
The generator $G_{\theta_G}$ is a function that maps elements from a latent distribution $p_z$ to $p_{fake}$.
During training, it tries to adjust its internal parameters $\theta_G$ such that $p_{fake}$ resembles $p_{real}$, and thus tries to fool the discriminator into thinking that the data produced by it belongs to $p_{real}$.

The discriminator and generator can be thought of as two players playing a game with value function $V(D, G)$ against each other, where
\Todo{Think about what happens when $D(G(z))=1$}
\begin{equation}
	V(D, G) = \mathbb{E}_{r\sim p_{real}}(\log(D(r))) + \mathbb{E}_{z\sim p_{z}}(\log(1 - D(G(z)))) \ .
\end{equation}
Here, the discriminator tries to maximize the probability of correctly estimating whether the input stems from $p_{real}$ or $p_{fake}$.
The generator simultaneously minimizes the second summand, trying to achieve the same discriminator output as for data from $p_{real}$.
Thus, GANs can be described as a minimax game with players $G$ and $D$:
\begin{equation}
\min_G \max_D V(D, G) \ .
\end{equation}
\citet{goodfellow14} have shown that this expression takes on its global optimum if and only $p_{fake} = p_{real}$.
In this case, the discriminator produces an output of $\frac{1}{2}$ for all inputs, which means it is no longer able to distinguish between the distributions.

The advantage of GANs compared to other generative models is that they can learn distributions in a weakly supervised manner, that is, only elements from the distribution to be learned are required from training.
In the system presented by \citet{drover18} that distribution captures all human \emph{2D} poses, which will be further explained in the following sections.

\subsection{Perspective Projection}

\input{figures/camera_projection_figure}

Throughout this thesis, human poses will be projected from three dimensional space into two dimensions and vice versa.
For this "photography" the simplified model of a pinhole camera is used whose only intrinsic parameter, in this case, is a focal length $f$.
The camera's projection from 3D to 2D can be described by the perspective projection equation:
\begin{equation}
	\label{eq:perspective-projection}
	p = 
	\begin{pmatrix}
	x\\
	y
	\end{pmatrix}
	= \frac{f}{Z} \cdot 	
	\begin{pmatrix}
	X\\
	Y
	\end{pmatrix} \ ,
\end{equation}
where $P = (X, Y, Z)$ is a is a three dimensional point in the camera's coordinate system and $p = (x, y)$ the projected point on the (virtual) image plane.
Note that the projection is only defined for points that have $Z \neq 0$.
$Z = 0$ would mean the point is in the same x-y-plane as the focal point/ the center of projection.
However, this is not a problem since it will always be ensured that the there is enough distance between the 3D points and the camera.
A visualization of the projection is depicted in \autoref{fig:camera-projection-setup}.

Similarly to the projection from three dimensions into two, a 2D point on the image plane $(x, y)$ can also be re-projected into 3D if the depth $Z$ of the point is given.
This re-projection is described by
\begin{equation}
	\label{eq:perspective-re-projection}
	\begin{pmatrix}
	X\\
	Y
	\end{pmatrix} = \frac{Z}{f} \cdot
	\begin{pmatrix}
	x\\
	y
	\end{pmatrix} \ ,
\end{equation}
and the 3D point $P$ in then given by $(X, Y, Z)$.

\subsection{A GAN for 3D Pose Estimation}

\input{figures/model_figure}

\autoref{fig:system} displays a diagram of this system.

For numerical stability, \citet{drover18} require the input 2D poses (the poses to be lifted to 3D) to be normalized in the following way:
\begin{enumerate}[label=(\Alph*)]
	\item The 2D poses' root joint is centered at the origin of the image plane.
	\item A designated norm limb has length $0.1$.
\end{enumerate}
In this work, the root joint always is the pelvis (the center point between the hips) and the norm limb the connection (or distance) between pelvis and thorax.

In the system, the latent and the real distribution $p_z$ and $p_real$ are both the distribution of real 2D poses.
This means that the generator is also trying to learn that distribution.
On the way to doing that, 3D human poses are created as a byproduct, which can then be used in other applications.

In the proposed system, the generator receives 2D poses as input.
For those poses, it estimates the depth of each joint.
In order to eliminate the additional degree of freedom introduced by the scale-distance ambiguity (a 3D pose $P$ and a 3D pose $P'$ twice as far away from the camera and twice as big both project to the same 2D pose), the system aims to estimate 3D poses that have a norm limb length of $1$.
Hence, instead of an absolute depth, only a \emph{depth offset} $o_i$ is estimated for each joint $(x_i, y_i)$.
The absolute depths can then be calculated as
\begin{equation}
	\label{eq:depth-clipping}
	Z_i = \max \{f, Z + o_i\} \ .
\end{equation}
The clipping ensures that, at the time of re-projection, the points are projected in front of the camera.

Using the so found depths for each joint, the 2D pose can be re-projected into three dimensions using \autoref{eq:perspective-re-projection}.
In practice, the $Z$ in \autoref{eq:depth-clipping} is $10$, such that the re-projected poses have an approximate norm limb length of $1$.

Afterwards, the obtained 3D poses are fed into a \emph{Random Projection Layer}, where they are projected into two dimensions again.
For this, the poses are randomly rotated with with azimuth angles between $0$ and $360$ degrees and elevation angles between $0$ and $20$ degrees, first and then projected like in \autoref{eq:perspective-projection}.
Subsequently, those 2D poses are fed to the discriminator.


The \textit{discriminator} $D$ is defined as a neural network that consumes real 2D poses from the distribution $P_{real}$ and fake 2D poses drawn from $P_{fake}$ and outputs a scalar value in $[0, 1]$.

The loss functions used for the generator and the discriminator are the following \cite{goodfellow17}:
\begin{align}
	\label{eq:generator-loss}
	loss_G &= -\mathbb{E}_{p\sim P_{fake}}(\log(D(p))) \\
	\label{eq:discriminator-loss}
	loss_D &= -\frac{1}{2}\mathbb{E}_{r\sim P_{real}}(\log(D(r))) - \frac{1}{2} \mathbb{E}_{p\sim P_{fake}}(\log(1 - D(p)))
\end{align}

Heuristic...


\Todo{When 2D poses are sampled, the extrinsic parameters of the synthetic camera model should match those of the real camera(s)}

\Todo[inline]{Figure of residual blocks}

Both generator and discriminator consist of several consecutive \textit{residual blocks}.
A residual block consists of two fully connected layers of size $1024$ each followed by RELU activation and a residual connection adding the input to the last layer's output.
\citet{drover18} suggest two additional Batch Normalization layers \cite{ioffe15}, one immediately after each fully connected layer.
\Todo{Mention correspondence with other authors and code.}
Practical tests have shown that when using those additional layers the training does not converge at all.
\unsure{Wording of "auf die Batch Norm wurde verzichtet"}
As competitive results could also be achieved with the reduced network design, Batch Normalization has been left out completely in this work.

The generator takes $n$ 2D joint locations (i.e. $2n$ scalar inputs) as input and feeds them to a fully connected layer of size 1024.
This is followed by four of the residual blocks described above and concluded by another fully connected layer of size 1024 mapping the output of the last residual block to a vector of size $n$ representing the depth offset of each joint.
The 2D input and the estimated depth offsets are then combined to a 3D pose as described above.
From each predicted 3D pose a random 2D projection is created that is used for training.

The discriminator's architecture is very similar to the generator's. 
It also accepts $n$ 2D joint locations as input which are fed into a fully connected layer of size 1024, followed by three residual blocks.
The output is then reduced by another fully connected layer to a vector of size 2.
When softmax is applied during the error calculation, the values will imply the probability of the input being real or generated.